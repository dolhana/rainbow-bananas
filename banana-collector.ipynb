{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# automatically reload python modules if there is a change\n",
    "# See https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# matplotlib plots are embedded inside of the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Banana Collector\n",
    "\n",
    "This project demonstrates how to train an agent to collect bananas in a room using Deep Q-Networks algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Create an environment\n",
    "env = UnityEnvironment(file_name='Banana_Linux/Banana.x86')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BananaBrain']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brain_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainParameters at 0x7fc83909e550>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unityagents.brain.BrainParameters at 0x7fc83909e550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_name = 'BananaBrain'\n",
    "brain = env.brains[brain_name]\n",
    "brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "\n",
    "The agent can take one of 4 possible actions.\n",
    "\n",
    "- 0 - walk forward\n",
    "- 1 - walk backward\n",
    "- 2 - turn left\n",
    "- 3 - turn right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'discrete'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_action_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of possible actions\n",
    "n_actions = brain.vector_action_space_size\n",
    "n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity along with ray-based perception of objects around agent's forward direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'continuous'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.vector_observation_space_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of state dimensions\n",
    "n_state_dims = brain.vector_observation_space_size\n",
    "n_state_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainInfo at 0x7fc83909e048>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unityagents.brain.BrainInfo at 0x7fc83909eef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is only one available brain\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        , 0.16895212,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.20073597,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.12865657,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.14938059,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.58185619,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.16089135,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.31775284,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environment using a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# Get the current state\n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "# Initialize the score\n",
    "score = 0\n",
    "\n",
    "# Keep exploring until done == True\n",
    "done = False\n",
    "while not done:\n",
    "    \n",
    "    # Select a random action\n",
    "    action = np.random.randint(n_actions)\n",
    "    \n",
    "    # Take the action\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    \n",
    "    # Observe the result\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    \n",
    "    # Accumulate the reward to the score\n",
    "    score += reward\n",
    "    \n",
    "    # Prepare the state for the next action\n",
    "    state = next_state\n",
    "    \n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.04178659,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.21819802,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.39524487,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.58369923,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.21722141,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "Loss is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{DQN} = (R_{t+1} + \\gamma_{t+1} \\max_{a'}{q_{\\bar{\\theta}}}(S_{t+1},a') - q_\\theta(S_t,A_t))^2,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "  * $t$ : a time step randomly picked from the replay memory\n",
    "  * $\\theta$ : the parameters of the _online network_\n",
    "  * $\\bar{\\theta}$ : the parameters of the _target network_\n",
    "\n",
    "Notes:\n",
    "  * The gradient of the loss is back-propagated only into $\\theta$.\n",
    "  * $\\theta$ is periodically copied to $\\bar{\\theta}$.\n",
    "  * Mini-batches are sampled uniformly from the experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-learning\n",
    "\n",
    "Double Q-learning addresses the overestimation of DQN by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation.\n",
    "\n",
    "Double Q-learning defines the loss as:\n",
    "\n",
    "$$\n",
    "L_{DDQN} = (R_{t+1} + \\gamma_{t+1} q_{\\bar{\\theta}}(S_{t+1},\\argmax_{a'}{q_\\theta (S_{t+q},a')}) - q_\\theta(S_t,A_t))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay\n",
    "\n",
    "Prioritized experence replay samples transitions with probability $p_t$ relative to the last encountered absolute _TD error_:\n",
    "\n",
    "$$\n",
    "p_t \\propto |R_{t+1} + \\gamma_{t+1} \\max_{a'} q_{\\bar{\\theta}}(S_{t+1},a') - q_\\theta(S_t,A_t)|^w,\n",
    "$$\n",
    "\n",
    "where $w$ is a hyper-parameter that determines the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-step learning\n",
    "\n",
    "A multi-step variant of DQN uses foward-view _multi-step_ targets and the alternative loss, which is defined as:\n",
    "\n",
    "$$\n",
    "L_{multi-step} = (R_t^{(n)} + \\gamma_t^{(n)} \\max_{a'} q_{\\bar{\\theta}}(S_{t+n},a') - q_\\theta(S_t,A_t))^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R_t^{(n)} \\equiv \\sum_{k=0}^{n-1} \\gamma_t^{(k)} R_{t+k+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
