{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# automatically reload python modules if there is a change\n",
    "# See https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# matplotlib plots are embedded inside of the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Banana Collector\n",
    "\n",
    "This project demonstrates how to train an agent to collect bananas in a room using Deep Q-Networks algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The environment is a modified version of Unity ML-Agents [Banana-Collector][banana-collector].\n",
    "\n",
    "[banana-collector]: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal of the agent is to collect yellow bananas though avoiding blue bananas. The environment is considered to be solved when the average return for the consecutive 100 episode is over 13.\n",
    "\n",
    "### Reward\n",
    "\n",
    "The agent gets +1 reward when it reaches a yellow banana and -1 when it does a blue one.\n",
    "\n",
    "* +1 - yellow banana\n",
    "* -1 - blue banana\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation space has 37 dimensions and contains the agent's velocity plus ray-based perception of objects around the agent's forward direction.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "Based on the observation, the agent needs to learn how to best select actions. Four discrete actions are available:\n",
    "\n",
    "* 0 - move forward\n",
    "* 1 - move backward\n",
    "* 2 - turn left\n",
    "* 3 - turn right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Creating an environment\n",
    "env = UnityEnvironment('Banana_Windows_x86_64/Banana.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainParameters at 0x213ea1f1048>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BananaBrain']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.brain_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unityagents.brain.BrainParameters at 0x213ea1f1048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_name = 'BananaBrain'\n",
    "brain = env.brains[brain_name]\n",
    "brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "Loss is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{DQN} = (R_{t+1} + \\gamma_{t+1} \\max_{a'}{q_{\\bar{\\theta}}}(S_{t+1},a') - q_\\theta(S_t,A_t))^2,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "  * $t$ : a time step randomly picked from the replay memory\n",
    "  * $\\theta$ : the parameters of the _online network_\n",
    "  * $\\bar{\\theta}$ : the parameters of the _target network_\n",
    "\n",
    "Notes:\n",
    "  * The gradient of the loss is back-propagated only into $\\theta$.\n",
    "  * $\\theta$ is periodically copied to $\\bar{\\theta}$.\n",
    "  * Mini-batches are sampled uniformly from the experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-learning\n",
    "\n",
    "Double Q-learning addresses the overestimation of DQN by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation.\n",
    "\n",
    "Double Q-learning defines the loss as:\n",
    "\n",
    "$$\n",
    "L_{DDQN} = (R_{t+1} + \\gamma_{t+1} q_{\\bar{\\theta}}(S_{t+1},\\argmax_{a'}{q_\\theta (S_{t+q},a')}) - q_\\theta(S_t,A_t))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay\n",
    "\n",
    "Prioritized experence replay samples transitions with probability $p_t$ relative to the last encountered absolute _TD error_:\n",
    "\n",
    "$$\n",
    "p_t \\propto |R_{t+1} + \\gamma_{t+1} \\max_{a'} q_{\\bar{\\theta}}(S_{t+1},a') - q_\\theta(S_t,A_t)|^w,\n",
    "$$\n",
    "\n",
    "where $w$ is a hyper-parameter that determines the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-step learning\n",
    "\n",
    "A multi-step variant of DQN uses foward-view _multi-step_ targets and the alternative loss, which is defined as:\n",
    "\n",
    "$$\n",
    "L_{multi-step} = (R_t^{(n)} + \\gamma_t^{(n)} \\max_{a'} q_{\\bar{\\theta}}(S_{t+n},a') - q_\\theta(S_t,A_t))^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R_t^{(n)} \\equiv \\sum_{k=0}^{n-1} \\gamma_t^{(k)} R_{t+k+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
