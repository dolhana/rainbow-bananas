{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# automatically reload python modules if there is a change\n",
    "# See https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# matplotlib plots are embedded inside of the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Banana Collector\n",
    "\n",
    "This project demonstrates how to train an agent to collect bananas in a room using Deep Q-Networks algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "Loss is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{DQN} = (R_{t+1} + \\gamma_{t+1} \\max_{a'}{q_{\\bar{\\theta}}}(S_{t+1},a') - q_\\theta(S_t,A_t))^2,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "  * $t$ : a time step randomly picked from the replay memory\n",
    "  * $\\theta$ : the parameters of the _online network_\n",
    "  * $\\bar{\\theta}$ : the parameters of the _target network_\n",
    "\n",
    "Notes:\n",
    "  * The gradient of the loss is back-propagated only into $\\theta$.\n",
    "  * $\\theta$ is periodically copied to $\\bar{\\theta}$.\n",
    "  * Mini-batches are sampled uniformly from the experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-learning\n",
    "\n",
    "Double Q-learning addresses the overestimation of DQN by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation.\n",
    "\n",
    "Double Q-learning defines the loss as:\n",
    "\n",
    "$$\n",
    "L_{DDQN} = (R_{t+1} + \\gamma_{t+1} q_{\\bar{\\theta}}(S_{t+1},\\argmax_{a'}{q_\\theta (S_{t+q},a')}) - q_\\theta(S_t,A_t))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay\n",
    "\n",
    "Prioritized experence replay samples transitions with probability $p_t$ relative to the last encountered absolute _TD error_:\n",
    "\n",
    "$$\n",
    "p_t \\propto |R_{t+1} + \\gamma_{t+1} \\max_{a'} q_{\\bar{\\theta}}(S_{t+1},a') - q_\\theta(S_t,A_t)|^w,\n",
    "$$\n",
    "\n",
    "where $w$ is a hyper-parameter that determines the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-step learning\n",
    "\n",
    "A multi-step variant of DQN uses foward-view _multi-step_ targets and the alternative loss, which is defined as:\n",
    "\n",
    "$$\n",
    "L_{multi-step} = (R_t^{(n)} + \\gamma_t^{(n)} \\max_{a'} q_{\\bar{\\theta}}(S_{t+n},a') - q_\\theta(S_t,A_t))^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R_t^{(n)} \\equiv \\sum_{k=0}^{n-1} \\gamma_t^{(k)} R_{t+k+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
